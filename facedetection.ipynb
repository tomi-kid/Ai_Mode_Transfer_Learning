{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOycicK9zmJHWzOaK46KzQo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "ir",
      "display_name": "R"
    },
    "language_info": {
      "name": "R"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tomi-kid/Ai_Mode_Transfer_Learning/blob/main/facedetection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[Edited by Tomas ketema college of computing and informatics:](https://t.me/fourhash)**"
      ],
      "metadata": {
        "id": "8He5ys3tbUeR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOTE:** The 2,000 images used in this exercise are excerpted from the [\"Dogs vs. Cats\" dataset](https://www.kaggle.com/c/dogs-vs-cats/data) available on Kaggle, which contains 25,000 images. Here, we use a subset of the full dataset to decrease training time for educational purposes.\n"
      ],
      "metadata": {
        "id": "yrM4lK_QbOBM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip \\\n",
        "    -O /tmp/cats_and_dogs_filtered.zip\n",
        "    import os\n",
        "import zipfile\n",
        "\n",
        "local_zip = '/tmp/cats_and_dogs_filtered.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/tmp')\n",
        "zip_ref.close()\n",
        "base_dir = '/tmp/cats_and_dogs_filtered'\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "validation_dir = os.path.join(base_dir, 'validation')\n",
        "\n",
        "# Directory with our training cat pictures\n",
        "train_cats_dir = os.path.join(train_dir, 'cats')\n",
        "\n",
        "# Directory with our training dog pictures\n",
        "train_dogs_dir = os.path.join(train_dir, 'dogs')\n",
        "\n",
        "# Directory with our validation cat pictures\n",
        "validation_cats_dir = os.path.join(validation_dir, 'cats')\n",
        "\n",
        "# Directory with our validation dog pictures\n",
        "validation_dogs_dir = os.path.join(validation_dir, 'dogs')\n",
        "train_cat_fnames = os.listdir(train_cats_dir)\n",
        "print(train_cat_fnames[:10])\n",
        "\n",
        "train_dog_fnames = os.listdir(train_dogs_dir)\n",
        "train_dog_fnames.sort()\n",
        "print(train_dog_fnames[:10])\n",
        "print('total training cat images:', len(os.listdir(train_cats_dir)))\n",
        "print('total training dog images:', len(os.listdir(train_dogs_dir)))\n",
        "print('total validation cat images:', len(os.listdir(validation_cats_dir)))\n",
        "print('total validation dog images:', len(os.listdir(validation_dogs_dir)))"
      ],
      "metadata": {
        "id": "AUSSkt87b5T0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "# Parameters for our graph; we'll output images in a 4x4 configuration\n",
        "nrows = 4\n",
        "ncols = 4\n",
        "\n",
        "# Index for iterating over images\n",
        "pic_index = 0"
      ],
      "metadata": {
        "id": "JEIcxYsOcE7t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up matplotlib fig, and size it to fit 4x4 pics\n",
        "fig = plt.gcf()\n",
        "fig.set_size_inches(ncols * 4, nrows * 4)\n",
        "\n",
        "pic_index += 8\n",
        "next_cat_pix = [os.path.join(train_cats_dir, fname)\n",
        "                for fname in train_cat_fnames[pic_index-8:pic_index]]\n",
        "next_dog_pix = [os.path.join(train_dogs_dir, fname)\n",
        "                for fname in train_dog_fnames[pic_index-8:pic_index]]\n",
        "\n",
        "for i, img_path in enumerate(next_cat_pix+next_dog_pix):\n",
        "  # Set up subplot; subplot indices start at 1\n",
        "  sp = plt.subplot(nrows, ncols, i + 1)\n",
        "  sp.axis('Off') # Don't show axes (or gridlines)\n",
        "\n",
        "  img = mpimg.imread(img_path)\n",
        "  plt.imshow(img)\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "o7WGLbSXcHcs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import Model\n",
        "# Our input feature map is 150x150x3: 150x150 for the image pixels, and 3 for\n",
        "# the three color channels: R, G, and B\n",
        "img_input = layers.Input(shape=(150, 150, 3))\n",
        "\n",
        "# First convolution extracts 16 filters that are 3x3\n",
        "# Convolution is followed by max-pooling layer with a 2x2 window\n",
        "x = layers.Conv2D(16, 3, activation='relu')(img_input)\n",
        "x = layers.MaxPooling2D(2)(x)\n",
        "\n",
        "# Second convolution extracts 32 filters that are 3x3\n",
        "# Convolution is followed by max-pooling layer with a 2x2 window\n",
        "x = layers.Conv2D(32, 3, activation='relu')(x)\n",
        "x = layers.MaxPooling2D(2)(x)\n",
        "\n",
        "# Third convolution extracts 64 filters that are 3x3\n",
        "# Convolution is followed by max-pooling layer with a 2x2 window\n",
        "x = layers.Conv2D(64, 3, activation='relu')(x)\n",
        "x = layers.MaxPooling2D(2)(x)"
      ],
      "metadata": {
        "id": "iQhu9V63cKlc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Flatten feature map to a 1-dim tensor so we can add fully connected layers\n",
        "x = layers.Flatten()(x)\n",
        "\n",
        "# Create a fully connected layer with ReLU activation and 512 hidden units\n",
        "x = layers.Dense(512, activation='relu')(x)\n",
        "\n",
        "# Create output layer with a single node and sigmoid activation\n",
        "output = layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "# Create model:\n",
        "# input = input feature map\n",
        "# output = input feature map + stacked convolution/maxpooling layers + fully\n",
        "# connected layer + sigmoid output layer\n",
        "model = Model(img_input, output)\n",
        "model.summary()\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=RMSprop(lr=0.001),\n",
        "              metrics=['acc'])\n",
        "              from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# All images will be rescaled by 1./255\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Flow training images in batches of 20 using train_datagen generator\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        train_dir,  # This is the source directory for training images\n",
        "        target_size=(150, 150),  # All images will be resized to 150x150\n",
        "        batch_size=20,\n",
        "        # Since we use binary_crossentropy loss, we need binary labels\n",
        "        class_mode='binary')\n",
        "\n",
        "# Flow validation images in batches of 20 using val_datagen generator\n",
        "validation_generator = val_datagen.flow_from_directory(\n",
        "        validation_dir,\n",
        "        target_size=(150, 150),\n",
        "        batch_size=20,\n",
        "        class_mode='binary')"
      ],
      "metadata": {
        "id": "cr-srEnncPnE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training the model\n",
        "Let's train on all 2,000 images available, for 15 epochs, and validate on all 1,000 validation images. (This may take a few minutes to run.)"
      ],
      "metadata": {
        "id": "1UjAHN_ncbG8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit_generator(\n",
        "      train_generator,\n",
        "      steps_per_epoch=100,  # 2000 images = batch_size * steps\n",
        "      epochs=15,\n",
        "      validation_data=validation_generator,\n",
        "      validation_steps=50,  # 1000 images = batch_size * steps\n",
        "      verbose=2)"
      ],
      "metadata": {
        "id": "6iiVEmy6ce61"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "HxoO48xochps"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**This code is not finished**\n"
      ],
      "metadata": {
        "id": "e6pqAh4MckR2"
      }
    }
  ]
}